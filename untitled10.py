# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fGMzTLIjEzNahgjtbgeD4Ef1uC8amN7z
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("heart.csv")

df.head()

df.info()

df.columns

df.isnull().sum()

import matplotlib.pyplot as plt
df.boxplot(figsize=(12,6))
plt.show()

df["sex"].unique()

print(df["cp"].unique())
print(df["cp"].value_counts())

df = pd.get_dummies(df, columns=['cp'], prefix='cp', drop_first=True)

df.head()

print(df["restecg"].unique())
print(df["restecg"].value_counts())

df = pd.get_dummies(df, columns=['restecg'], prefix='restecg', drop_first=True)

df.head()

df[['cp_1', 'cp_2', 'cp_3']].dtypes

print(df['slope'].unique())
print(df['slope'].value_counts())

df = pd.get_dummies(df, columns=['slope'], prefix='slope', drop_first=True)

df.head()

print(df['ca'].unique())
print(df['ca'].value_counts())

mode_ca = df['ca'].mode()[0]
df.loc[df['ca'] == 4, 'ca'] = mode_ca

print(df['ca'].value_counts())

df = pd.get_dummies(df, columns=['ca'], prefix='ca', drop_first=True)

df.head()

print(df['thal'].unique())
print(df['thal'].value_counts())

mode_thal = df['thal'].mode()[0]
df.loc[df['thal'] == 0, 'thal'] = mode_thal
print(df['thal'].value_counts())

df = pd.get_dummies(df, columns=['thal'], prefix='thal', drop_first=True)

df.head()

num_scaling_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']



df[num_cols].head()

X = df.drop(columns=['target'])
y = df['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train[num_scaling_cols] = scaler.fit_transform(X_train[num_scaling_cols])
X_test[num_scaling_cols] = scaler.transform(X_test[num_scaling_cols])

y_train.value_counts()
y_test.value_counts()

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(max_iter=500)

lr_model.fit(X_train, y_train)

y_pred_lr = lr_model.predict(X_test)

from sklearn.metrics import accuracy_score

acc_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Accuracy:", acc_lr)

df.drop_duplicates(inplace=True)
print(f"Yinelenen satırlar kaldırıldıktan sonraki veri seti boyutu: {df.shape[0]}")

X = df.drop(columns=['target'])
y = df['target']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train[num_scaling_cols] = scaler.fit_transform(X_train[num_scaling_cols])
X_test[num_scaling_cols] = scaler.transform(X_test[num_scaling_cols])
print("Veriler başarıyla yeniden ölçeklendirildi.")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

lr_model = LogisticRegression(max_iter=500)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
acc_lr = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Accuracy:", acc_lr)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    random_state=42
)

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)
print("Random Forest Accuracy:", acc_rf)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

from tensorflow.keras import layers, models
import tensorflow as tf

def build_mlp_model(optimizer):
    model = models.Sequential([
        layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
        layers.Dropout(0.2),

        layers.Dense(16, activation='relu'),
        layers.Dropout(0.2),

        layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )
    return model

early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-5
)

optimizer_nadam = tf.keras.optimizers.Nadam(learning_rate=0.001)

model_nadam = build_mlp_model(optimizer_nadam)

history_nadam = model_nadam.fit(
    X_train, y_train,
    epochs=150,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stop, reduce_lr],
    verbose=0
)

loss_nadam, acc_nadam, auc_nadam = model_nadam.evaluate(X_test, y_test, verbose=0)

print("Nadam Accuracy:", acc_nadam)
print("Nadam AUC:", auc_nadam)

optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.001)

model_adam = build_mlp_model(optimizer_adam)

history_adam = model_adam.fit(
   X_train, y_train,
    epochs=150,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stop, reduce_lr],
    verbose=0
)

loss_adam, acc_adam, auc_adam = model_adam.evaluate(X_test, y_test, verbose=0)

print("Adam Accuracy:", acc_adam)
print("Adam AUC:", auc_adam)

# Adam + BS=64
optimizer_adam64 = tf.keras.optimizers.Adam(learning_rate=0.001)

model_adam2 = build_mlp_model(optimizer_adam64)

history_adam_bs64 = model_adam2.fit(
    X_train, y_train,
    epochs=150,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=0
)

loss, acc, auc = model_adam2.evaluate(X_test, y_test, verbose=0)
print("Adam (BS=64):", acc, auc)

# Adam + BS=16
optimizer_adam16 = tf.keras.optimizers.Adam(learning_rate=0.001)

model_adam2 = build_mlp_model(optimizer_adam16)

history_adam_bs64 = model_adam2.fit(
    X_train, y_train,
    epochs=150,
    batch_size=49,
    validation_data=(X_test, y_test),
    verbose=0
)

loss, acc, auc = model_adam2.evaluate(X_test, y_test, verbose=0)
print("Adam (BS=64):", acc, auc)

optimizer_rms = tf.keras.optimizers.RMSprop(learning_rate=0.001)

model_rms = build_mlp_model(optimizer_rms)

history_rms = model_rms.fit(
   X_train, y_train,
    epochs=150,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stop, reduce_lr],
    verbose=0
)

loss_rms, acc_rms, auc_rms = model_rms.evaluate(X_test, y_test, verbose=0)

print("RMSprop Accuracy:", acc_rms)
print("RMSprop AUC:", auc_rms)

y_pred_ann = (model.predict(X_test) > 0.5).astype(int)

print("\n--- Optimizer Comparison ---")
print(f"Adam     -> Accuracy: {acc_adam:.4f} | AUC: {auc_adam:.4f}")
print(f"Nadam    -> Accuracy: {acc_nadam:.4f} | AUC: {auc_nadam:.4f}")
print(f"RMSprop  -> Accuracy: {acc_rms:.4f} | AUC: {auc_rms:.4f}")

from sklearn.metrics import roc_auc_score

# Logistic Regression AUC
auc_lr = roc_auc_score(y_test, lr_model.predict_proba(X_test)[:, 1])

# Random Forest AUC
auc_rf = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])

results = {
    "Model": [
        "Logistic Regression",
        "Random Forest",
        "MLP (Adam)",
        "MLP (Nadam)",
        "MLP (RMSprop)"
    ],
    "Accuracy": [
        acc_lr,
        acc_rf,
        acc_adam,
        acc_nadam,
        acc_rms
    ],
    "AUC": [
        auc_lr,
        auc_rf,
        auc_adam,
        auc_nadam,
        auc_rms
    ]
}

df_results = pd.DataFrame(results)
df_results